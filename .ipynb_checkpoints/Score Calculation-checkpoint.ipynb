{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import datetime as dt\n",
    "#import datetime as datetime\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from imblearn.ensemble import EasyEnsemble\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from imblearn.ensemble import EasyEnsemble\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read dataset and create more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv',sep=',')\n",
    "dataset.head()\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def getStdDate(x):\n",
    "    dt_list = x.split('/')\n",
    "    #dt_str = \"0\"+dt_list[0]+\"/\"+\"0\"+dt_list[1]+\"/\"+dt_list[2]\n",
    "    dt_str = dt_list[2]+\"-\"+\"0\"+dt_list[0]+\"-\"+dt_list[1]\n",
    "    datetime_object = dt.datetime.strptime(dt_str,'%Y-%m-%d')\n",
    "    return datetime_object\n",
    "\n",
    "dataset['creationdate'] = dataset['creationdate'].apply(lambda x : getStdDate(x))\n",
    "today= dt.datetime.today().strftime('%Y-%m-%d')\n",
    "today= pd.to_datetime(today)\n",
    "\n",
    "dataset['recency'] = dataset['creationdate'].apply(lambda x:(1/(today-x).days)*365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_count = dataset['bin'].value_counts()\n",
    "bin_count_pdf = bin_count.to_frame()\n",
    "bin_count_pdf['count_of_txn'] = bin_count_pdf['bin']\n",
    "bin_count_pdf['bin'] = bin_count_pdf.index\n",
    "bin_count_pdf.sort_values(by='count_of_txn',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sub = dataset[['bin','fraud']]\n",
    "dataset_sub_pdf = dataset_sub.groupby([\"bin\"]).sum().reset_index()\n",
    "dataset_sub_pdf.sort_values(by='bin').head()\n",
    "bin_fraud_pdf = bin_count_pdf.merge(dataset_sub_pdf, left_on='bin', right_on='bin', how='inner')\n",
    "bin_fraud_pdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_fraud_pdf['pc_fraud']=bin_fraud_pdf['fraud']/bin_fraud_pdf['count_of_txn']\n",
    "bin_fraud_pdf.sort_values(by='pc_fraud').head(5)\n",
    "bin_fraud_pdf= bin_fraud_pdf.rename(columns={\"fraud\": \"fraud_count\"})\n",
    "bin_fraud_pdf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combine the newly created features with the original dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.merge(bin_fraud_pdf, left_on='bin', right_on='bin', how='inner')\n",
    "dataset = dataset.drop_duplicates()\n",
    "print(\"Shape after join \",dataset.shape)\n",
    "\n",
    "#drop_list = ['creationdate','bin','count_of_txn','fraud_count']\n",
    "drop_list = ['creationdate','bin','fraud_refusal','declined','count_of_txn','fraud_count','amount','recency','pc_fraud']\n",
    "dataset = dataset.drop(drop_list,axis=1)\n",
    "\n",
    "y_col = 'fraud'\n",
    "X = dataset.drop(y_col,axis=1)\n",
    "#dataset['amount'] = dataset['amount'].apply(lambda x : np.log(x+1))\n",
    "y = dataset['fraud']\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove rows with least variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression,mutual_info_regression,chi2\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=(.98 * (1-.98)))\n",
    "sel.fit_transform(X)\n",
    "X.columns[sel.get_support(indices=True)]\n",
    "vc_list = X.columns[sel.get_support(indices=True)].tolist()\n",
    "print(\"Features selected after variance thresholding \\n\",vc_list)\n",
    "\n",
    "selector = SelectKBest(chi2, k=5)\n",
    "selector.fit(X, y)\n",
    "X_new = selector.transform(X)\n",
    "X_new.shape\n",
    "X.columns[selector.get_support(indices=True)]\n",
    "chi2_list = X.columns[selector.get_support(indices=True)].tolist()\n",
    "print(\"Chi2 List : \\n\",chi2_list)\n",
    "\n",
    "selector = SelectKBest(f_regression, k=5)\n",
    "selector.fit(X, y)\n",
    "X_new = selector.transform(X)\n",
    "X_new.shape\n",
    "X.columns[selector.get_support(indices=True)]\n",
    "f_classif_list = X.columns[selector.get_support(indices=True)].tolist()\n",
    "print(\"F Classify List: \\n\",f_classif_list)\n",
    "\n",
    "var_sel = vc_list+chi2_list+f_classif_list\n",
    "\n",
    "# X = X[ X.columns.intersection(var_sel)]\n",
    "# y= y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA to get sense of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_sc = scaler.fit_transform(X)\n",
    "\n",
    "# scaler = Normalizer().fit(X)\n",
    "# X = scaler.transform(X)\n",
    "\n",
    "target_names = [0,1]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X_sc).transform(X_sc)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "colors = ['navy', 'red']\n",
    "lw = 2\n",
    "for color, i, target_name in zip(colors, [0, 1], target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.3, lw=lw,label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of Fraud dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Imbalance of the classes\")\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def writeFeatureImportance(msg,model,auc,kappa):\n",
    "    \"\"\"\n",
    "    This function ,based on the auc and kappa score writes the features importance to \n",
    "    featureImportance.txt.\n",
    "    \"\"\"\n",
    "    if(auc>=0.72) and (kappa>=0.05 ):\n",
    "        fh = open(\"featureImportance.txt\",\"a\")\n",
    "        feature_list = ''.join(str(model.feature_importances_.tolist()).replace('[','').replace(']',''))\n",
    "        fh.write(msg+\",\"+feature_list+\"\\n\") \n",
    "        fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple classifier with no sampling of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "validation_size = 0.30\n",
    "seed = 13\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=validation_size,random_state=seed)\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "\n",
    "models = []\n",
    "models.append(( 'LR' , LogisticRegression(class_weight='balanced')))\n",
    "models.append(( 'DT' , DecisionTreeClassifier(class_weight='balanced')))\n",
    "models.append(( 'RF' , RandomForestClassifier(class_weight='balanced')))\n",
    "models.append(( 'ET' , ExtraTreesClassifier(class_weight='balanced')))\n",
    "models.append(( 'GBT' , GradientBoostingClassifier(max_features='auto')))\n",
    "models.append(( 'ABT' , AdaBoostClassifier()))\n",
    "#models.append(( 'KNN' , KNeighborsClassifier(n_neighbors=3)))\n",
    "#models.append(( 'SVC' , SVC(kernel='rbf', probability=True)))\n",
    "#models.append(( 'VC' , VotingClassifier(estimators=models[:-1],voting='soft')))\n",
    "\n",
    "\n",
    "for name,model in models:\n",
    "    print(\"================== Results for model : \",name+\" =========================== \\n\")\n",
    "    model.fit(X_train_sc, y_train)\n",
    "    predictions_train = model.predict(X_train_sc)\n",
    "    print(\"Train ROC-AUC :\",roc_auc_score(y_train, predictions_train))\n",
    "    print(\"Train Cohen-Kappa :\",cohen_kappa_score(y_train, predictions_train))\n",
    "    print(\"Train Classification Report : \\n\",classification_report(y_train,predictions_train))\n",
    "    X_test_sc = scaler.transform(X_test)\n",
    "    predictions = model.predict(X_test_sc)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    print(\"Test ROC_AUC :\",auc)\n",
    "    kappa = cohen_kappa_score(y_test, predictions)\n",
    "    print(\"Test Cohen-Kappa :\",kappa)\n",
    "    print(\"Train Classification Report : \\n\",classification_report(y_test, predictions))\n",
    "    print(\"Test Confusion Matrix :\\n \")\n",
    "    cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=[0,1],title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "    writeFeatureImportance(\"Class Weights ,\"+name,model,auc,kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the dataset is imbalaced one, lets try to the following techinques to see, if we can get higher precision and recall using the following :\n",
    "#### 1) Ramdom Over Sampling\n",
    "#### 2) Random Under Sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Random Oversampling\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7)\n",
    "y_train, y_test = np.asarray(y_train),np.asarray(y_test)\n",
    "os =  RandomOverSampler()\n",
    "X_train_res, y_train_res = os.fit_sample(X_train, y_train)\n",
    "\n",
    "print (\"Distribution of class labels before resampling {}\".format(Counter(y_train)))\n",
    "print (\"Distribution of class labels after resampling {}\".format(Counter(y_train_res)))\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append(( 'LR' , LogisticRegression()))\n",
    "models.append(( 'DT' , DecisionTreeClassifier()))\n",
    "models.append(( 'RF' , RandomForestClassifier()))\n",
    "models.append(( 'ET' , ExtraTreesClassifier()))\n",
    "models.append(( 'GBT' , GradientBoostingClassifier()))\n",
    "models.append(( 'ABT' , AdaBoostClassifier()))\n",
    "#models.append(( 'KNN' , KNeighborsClassifier(n_neighbors=3)))\n",
    "#models.append(( 'SVC' , SVC(kernel='rbf', probability=True)))\n",
    "#models.append(( 'VC' , VotingClassifier(estimators=models[:-1],voting='soft')))\n",
    "\n",
    "for name,model in models:\n",
    "    print(\"================== Results for model : \",name+\" =========================== \\n\")\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    predictions_train = model.predict(X_train_res)\n",
    "    print(\"Train ROC-AUC :\",roc_auc_score(y_train_res, predictions_train))\n",
    "    print(\"Train Cohen-Kappa :\",cohen_kappa_score(y_train_res, predictions_train))\n",
    "    print(\"Train Classification Report : \\n\",classification_report(y_train_res,predictions_train))\n",
    "    predictions = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    print(\"Test ROC-AUC :\",auc)\n",
    "    kappa = cohen_kappa_score(y_test, predictions)\n",
    "    print(\"Test Cohen-Kappa :\",kappa)\n",
    "    print(\"Test Classification Report : \\n\",classification_report(y_test, predictions))\n",
    "    print(\"Test Confusion Matrix :\\n \")\n",
    "    cnf_matrix = confusion_matrix(y_test,predictions)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=[0,1],title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "    writeFeatureImportance(\"Random Over Sampling ,\"+name,model,auc,kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Random Oversampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.7)\n",
    "y_train, y_test = np.asarray(y_train),np.asarray(y_test)\n",
    "us =  RandomUnderSampler()\n",
    "X_train_res, y_train_res = us.fit_sample(X_train, y_train)\n",
    "\n",
    "print (\"Distribution of class labels before resampling {}\".format(Counter(y_train)))\n",
    "print (\"Distribution of class labels after resampling {}\".format(Counter(y_train_res)))\n",
    "\n",
    "\n",
    "models = []\n",
    "models.append(( 'LR' , LogisticRegression()))\n",
    "models.append(( 'DT' , DecisionTreeClassifier()))\n",
    "models.append(( 'RF' , RandomForestClassifier()))\n",
    "models.append(( 'ET' , ExtraTreesClassifier()))\n",
    "models.append(( 'GBT' , GradientBoostingClassifier()))\n",
    "models.append(( 'ABT' , AdaBoostClassifier()))\n",
    "#models.append(( 'KNN' , KNeighborsClassifier(n_neighbors=3)))\n",
    "#models.append(( 'SVC' , SVC(kernel='rbf', probability=True)))\n",
    "#models.append(( 'VC' , VotingClassifier(estimators=models[:-1],voting='soft')))\n",
    "\n",
    "for name,model in models:\n",
    "    print(\"================== Results for model : \",name+\" =========================== \\n\")\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    predictions_train = model.predict(X_train_res)\n",
    "    print(\"Train ROC-AUC :\",roc_auc_score(y_train_res, predictions_train))\n",
    "    print(\"Train Cohen-Kappa :\",cohen_kappa_score(y_train_res, predictions_train))\n",
    "    print(\"Train Classification Report : \\n\",classification_report(y_train_res,predictions_train))\n",
    "    predictions = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    print(\"Test ROC-AUC :\",auc)\n",
    "    kappa = cohen_kappa_score(y_test, predictions)\n",
    "    print(\"Test Cohen-Kappa :\",kappa)\n",
    "    print(\"Test Classification Report : \\n\",classification_report(y_test, predictions))\n",
    "    print(\"Test Confusion Matrix :\\n \")\n",
    "    cnf_matrix = confusion_matrix(y_test,predictions)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=[0,1],title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "    writeFeatureImportance(\"Random Under Sampler ,\"+name,model,auc,kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE - Takes awfully long time to train, hence skipping ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # Majority class : 0  /  Minority class : L \n",
    "# # Our goal is to maximise precision on the majority class and maximise recall on the minority class.\n",
    "\n",
    "# from imblearn.combine import SMOTETomek\n",
    "\n",
    "# X_smt = X[ X.columns.intersection(f_classif_list)]\n",
    "# y_smt = y\n",
    "\n",
    "# print(\"Smote data shape\",X_smt.shape)\n",
    "\n",
    "# print(\"Smote data shape\",y_smt.shape)\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_r, y_smt, train_size=0.7)\n",
    "\n",
    "# y_train, y_test = np.asarray(y_train), np.asarray(y_test)\n",
    "\n",
    "# os_us = SMOTETomek(ratio=0.5,k=5)\n",
    "# X_train_res, y_train_res = os_us.fit_sample(X_train, y_train)\n",
    "\n",
    "# print (\"Distribution of class labels before resampling {}\".format(Counter(y_train)))\n",
    "# print (\"Distribution of class labels after resampling {}\".format(Counter(y_train_res)))\n",
    "\n",
    "# #lr = LogisticRegression(n_jobs=10) \n",
    "# lr = ExtraTreesClassifier(n_estimators=100)\n",
    "# # #param_grid = { 'C': np.arange(1,11,1)}\n",
    "# # #CV_lr = RandomizedSearchCV(estimator=lr,param_distributions=param_grid,cv=10,scoring='f1_weighted',random_state=seed)\n",
    "# # CV_lr = lr\n",
    "# # CV_lr.fit(X_train_sc, y_train)\n",
    "# # #print (CV_lr.best_params_)\n",
    "# # #lr = CV_lr.best_estimator_\n",
    "\n",
    "# lr.fit(X_train_res, y_train_res)\n",
    "\n",
    "# predictions_train = lr.predict(X_train_res)\n",
    "# print(\"Train Classification Report : \\n\",classification_report(y_train_res,predictions_train))\n",
    "\n",
    "# X_test_sc = scaler.transform(X_test)\n",
    "# predictions = lr.predict(X_test_sc)\n",
    "\n",
    "# print(\"Test Classification Report : \\n\",classification_report(y_test, predictions))\n",
    "\n",
    "# print(\"Test Confusion Matrix :\\n \")\n",
    "# cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=[0,1],title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Near Miss is another under sampling method technique which gives a better seperation of class after separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "us = NearMiss(ratio=0.5,size_ngh=3, version=2)\n",
    "X_train_res, y_train_res = us.fit_sample(X_train, y_train)\n",
    "\n",
    "print (\"Distribution of class labels before resampling {}\".format(Counter(y_train)))\n",
    "print (\"Distribution of class labels after resampling {}\".format(Counter(y_train_res)))\n",
    "\n",
    "models = []\n",
    "models.append(( 'LR' , LogisticRegression()))\n",
    "models.append(( 'DT' , DecisionTreeClassifier()))\n",
    "models.append(( 'RF' , RandomForestClassifier()))\n",
    "models.append(( 'ET' , ExtraTreesClassifier()))\n",
    "models.append(( 'GBT' , GradientBoostingClassifier()))\n",
    "models.append(( 'ABT' , AdaBoostClassifier()))\n",
    "#models.append(( 'KNN' , KNeighborsClassifier(n_neighbors=3)))\n",
    "#models.append(( 'SVC' , SVC(kernel='rbf', probability=True)))\n",
    "#models.append(( 'VC' , VotingClassifier(estimators=models[:-1],voting='soft')))\n",
    "\n",
    "for name,model in models:\n",
    "    print(\"================== Results for model : \",name+\" =========================== \\n\")\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    predictions_train = model.predict(X_train_res)\n",
    "    print(\"Train ROC-AUC :\",roc_auc_score(y_train_res, predictions_train))\n",
    "    print(\"Train Cohen-Kappa :\",cohen_kappa_score(y_train_res, predictions_train))\n",
    "    print(\"Train Classification Report : \\n\",classification_report(y_train_res,predictions_train))\n",
    "    predictions = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    print(\"Test ROC-AUC :\",auc)\n",
    "    kappa = cohen_kappa_score(y_test, predictions)\n",
    "    print(\"Test Cohen-Kappa :\",kappa)\n",
    "    print(\"Test Classification Report : \\n\",classification_report(y_test, predictions))\n",
    "    print(\"Test Confusion Matrix :\\n \")\n",
    "    cnf_matrix = confusion_matrix(y_test,predictions)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=[0,1],title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "    writeFeatureImportance(\"Near Miss ,\"+name,model,auc,kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from imblearn.ensemble import EasyEnsemble\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import KFold, train_test_split\n",
    "from imblearn.ensemble import EasyEnsemble\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "y_train, y_test = np.asarray(y_train), np.asarray(y_test)\n",
    "ens = EasyEnsemble()\n",
    "X_train_res, y_train_res = ens.fit_sample(X_train, y_train)\n",
    "y_pred_proba = np.zeros(len(y_test))\n",
    "\n",
    "models = []\n",
    "models.append(( 'LR' , LogisticRegression()))\n",
    "models.append(( 'DT' , DecisionTreeClassifier()))\n",
    "models.append(( 'RF' , RandomForestClassifier()))\n",
    "models.append(( 'ET' , ExtraTreesClassifier()))\n",
    "models.append(( 'GBT' , GradientBoostingClassifier()))\n",
    "models.append(( 'ABT' , AdaBoostClassifier()))\n",
    "#models.append(( 'KNN' , KNeighborsClassifier(n_neighbors=3)))\n",
    "#models.append(( 'SVC' , SVC(kernel='rbf', probability=True)))\n",
    "#models.append(( 'VC' , VotingClassifier(estimators=models[:-1],voting='soft')))\n",
    "\n",
    "for name,model in models:\n",
    "    print(\"================== Results for model : \",name+\" =========================== \\n\")\n",
    "    for idx in range(len(y_train_res)):\n",
    "        clf_base = model\n",
    "        #grid = {'n_estimators': [50]}\n",
    "        cv = KFold(X_train_res.shape[0], n_folds=5, shuffle=True, random_state=0)\n",
    "        #clf = GridSearchCV(clf_base, grid, cv=cv, scoring='f1_weighted')\n",
    "        clf_base.fit(X_train_res[idx], y_train_res[idx])\n",
    "        y_pred_proba += list(zip(*clf_base.predict_proba(X_test)))[0]\n",
    "    y_pred_proba = y_pred_proba/len(y_train_res)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    print(\"Test ROC-AUC :\",auc)\n",
    "    kappa = cohen_kappa_score(y_test,y_pred)\n",
    "    print(\"Test Cohen-Kappa :\",kappa)\n",
    "    print (classification_report(y_test, y_pred))\n",
    "    print(\"Test Confusion Matrix :\\n \")\n",
    "    cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=[0,1],title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "    writeFeatureImportance(\"Easy Ensemble,\"+name,model,auc,kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "#### SMOTE with ENN , takes ages to train, hence skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "# y_train, y_test = np.asarray(y_train), np.asarray(y_test)\n",
    "\n",
    "# sm = SMOTEENN()\n",
    "# X_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n",
    "\n",
    "# print (\"Distribution of class labels before resampling {}\".format(Counter(y_train)))\n",
    "# print (\"Distribution of class labels after resampling {}\".format(Counter(y_train_res)))\n",
    "\n",
    "# #lr = LogisticRegression(n_jobs=10) \n",
    "# lr = ExtraTreesClassifier(n_estimators=100)\n",
    "# # #param_grid = { 'C': np.arange(1,11,1)}\n",
    "# # #CV_lr = RandomizedSearchCV(estimator=lr,param_distributions=param_grid,cv=10,scoring='f1_weighted',random_state=seed)\n",
    "# # CV_lr = lr\n",
    "# # CV_lr.fit(X_train_sc, y_train)\n",
    "# # #print (CV_lr.best_params_)\n",
    "# # #lr = CV_lr.best_estimator_\n",
    "\n",
    "# lr.fit(X_train_res, y_train_res)\n",
    "\n",
    "# predictions_train = lr.predict(X_train_res)\n",
    "# print(\"Train Classification Report : \\n\",classification_report(y_train_res,predictions_train))\n",
    "\n",
    "# X_test_sc = scaler.transform(X_test)\n",
    "# predictions = lr.predict(X_test_sc)\n",
    "\n",
    "# print(\"Test Classification Report : \\n\",classification_report(y_test, predictions))\n",
    "\n",
    "# print(\"Test Confusion Matrix :\\n \")\n",
    "# cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=[0,1],title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "# renn = RepeatedEditedNearestNeighbours(random_state=0)\n",
    "# X_train_res, y_train_res = renn.fit_sample(X_train, y_train)\n",
    "# # print(sorted(Counter(y_resampled).items()))\n",
    "# # X_train_res, y_train_res = us.fit_sample(X_train, y_train)\n",
    "# print (\"Distribution of class labels before resampling {}\".format(Counter(y_train)))\n",
    "# print (\"Distribution of class labels after resampling {}\".format(Counter(y_train_res)))\n",
    "# #lr = LogisticRegression(n_jobs=10) \n",
    "# lr = ExtraTreesClassifier(n_estimators=100)\n",
    "\n",
    "# # #param_grid = { 'C': np.arange(1,11,1)}\n",
    "# # #CV_lr = RandomizedSearchCV(estimator=lr,param_distributions=param_grid,cv=10,scoring='f1_weighted',random_state=seed)\n",
    "# # CV_lr = lr\n",
    "# # CV_lr.fit(X_train_sc, y_train)\n",
    "# # #print (CV_lr.best_params_)\n",
    "# # #lr = CV_lr.best_estimator_\n",
    "# lr.fit(X_train_res, y_train_res)\n",
    "# predictions_train = lr.predict(X_train_res)\n",
    "# print(\"Train Classification Report : \\n\",classification_report(y_train_res,predictions_train))\n",
    "# X_test_sc = scaler.transform(X_test)\n",
    "# predictions = lr.predict(X_test_sc)\n",
    "# print(\"Test Classification Report : \\n\",classification_report(y_test, predictions))\n",
    "# print(\"Test Confusion Matrix :\\n \")\n",
    "# cnf_matrix = confusion_matrix(y_test, predictions)\n",
    "# plt.figure()\n",
    "# plot_confusion_matrix(cnf_matrix, classes=[0,1],title='Confusion matrix, without normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Of all the sampling techniques used, Balanced Bagging classifer showed the most promise ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingClassifier,AdaBoostClassifier,RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier,GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from imblearn.datasets import make_imbalance\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "models = []\n",
    "models.append(( 'LR' , LogisticRegression(class_weight='balanced')))\n",
    "models.append(( 'DT' , DecisionTreeClassifier(class_weight='balanced')))\n",
    "models.append(( 'RF' , RandomForestClassifier(class_weight='balanced')))\n",
    "models.append(( 'ET' , ExtraTreesClassifier(class_weight='balanced')))\n",
    "models.append(( 'GBT' , GradientBoostingClassifier(max_features='auto')))\n",
    "models.append(( 'ABT' , AdaBoostClassifier()))\n",
    "\n",
    "for name,model in models:\n",
    "    print('Results of Balanced Bagging Classifier using Base Estimator :'+name+\"\\n\")\n",
    "    balanced_bagging = BalancedBaggingClassifier(random_state=0,n_estimators=250,base_estimator=model)\n",
    "    print('Class distribution of the training set: {}'.format(Counter(y_train)))\n",
    "    balanced_bagging.fit(X_train, y_train)\n",
    "    predictions_train = balanced_bagging.predict(X_train)\n",
    "    print(\"Train ROC-AUC :\",roc_auc_score(y_train, predictions_train))\n",
    "    print(\"Train Cohen-Kappa :\",cohen_kappa_score(y_train,predictions_train))\n",
    "    print('Class distribution of the test set: {}'.format(Counter(y_test)))\n",
    "    print('Classification results using a bagging classifier on balanced data')\n",
    "    y_pred_balanced_bagging = balanced_bagging.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred_balanced_bagging)\n",
    "    print(\"Test ROC-AUC :\",auc)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred_balanced_bagging)\n",
    "    print(\"Test Cohen-Kappa :\",kappa)\n",
    "    print(classification_report_imbalanced(y_test, y_pred_balanced_bagging))\n",
    "    cm_balanced_bagging = confusion_matrix(y_test, y_pred_balanced_bagging)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cm_balanced_bagging,classes=[0,1],title='Confusion matrix using BalancedBaggingClassifier')\n",
    "    plt.show()\n",
    "    writeFeatureImportance(\"Balanced Bagging Classifier,\"+name,model,auc,kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.over_sampling import ADASYN \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "ada = ADASYN(random_state=42)\n",
    "X_res, y_res = ada.fit_sample(X_train, y_train)\n",
    "X_train_res, y_train_res = ada.fit_sample(X_train, y_train)\n",
    "\n",
    "print (\"Distribution of class labels before resampling {}\".format(Counter(y_train)))\n",
    "print (\"Distribution of class labels after resampling {}\".format(Counter(y_train_res)))\n",
    "\n",
    "models = []\n",
    "models.append(( 'LR' , LogisticRegression()))\n",
    "models.append(( 'DT' , DecisionTreeClassifier()))\n",
    "models.append(( 'RF' , RandomForestClassifier()))\n",
    "models.append(( 'ET' , ExtraTreesClassifier()))\n",
    "models.append(( 'GBT' , GradientBoostingClassifier()))\n",
    "models.append(( 'ABT' , AdaBoostClassifier()))\n",
    "#models.append(( 'VC' , VotingClassifier(estimators=models[:-1],voting='hard',weights=[5,3,3,3,1])))\n",
    "\n",
    "for name,model in models:\n",
    "    print(\"================== Results for model : \",name+\" =========================== \\n\")\n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    predictions_train = model.predict(X_train_res)\n",
    "    print(\"Train ROC-AUC :\",roc_auc_score(y_train_res, predictions_train))\n",
    "    print(\"Train Cohen-Kappa :\",cohen_kappa_score(y_train_res, predictions_train))\n",
    "    print(\"Train Classification Report : \\n\",classification_report(y_train_res,predictions_train))\n",
    "    predictions = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "    print(\"Test ROC-AUC :\",auc)\n",
    "    kappa = cohen_kappa_score(y_test, predictions)\n",
    "    print(\"Test Cohen-Kappa :\",kappa)\n",
    "    print(\"Test Classification Report : \\n\",classification_report(y_test, predictions))\n",
    "    print(\"Test Confusion Matrix :\\n \")\n",
    "    cnf_matrix = confusion_matrix(y_test,predictions)\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(cnf_matrix, classes=[0,1],title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "    writeFeatureImportance(\"ADASYN,\"+name,model,auc,kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SMOTE is a distance based assumtion.\n",
    "In the higher dimension, it may not perform very well, hence we will do some dimensionality reduction using Kbest selection and using it to do a SMOTE  analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X_sc = scaler.fit_transform(X)\n",
    "target_names = [0,1]\n",
    "pca = PCA(n_components=2)\n",
    "X_r = pca.fit(X_sc).transform(X_sc)\n",
    "\n",
    "# Percentage of variance explained for each components\n",
    "print('explained variance ratio (first two components): %s'% str(pca.explained_variance_ratio_))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "colors = ['navy', 'red']\n",
    "lw = 2\n",
    "for color, i, target_name in zip(colors, [0, 1], target_names):\n",
    "    plt.scatter(X_r[y == i, 0], X_r[y == i, 1], color=color, alpha=.3, lw=lw,label=target_name)\n",
    "plt.legend(loc='best', shadow=False, scatterpoints=1)\n",
    "plt.title('PCA of Fraud dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes,scoring = 'roc_auc')\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Learning Curves (Extra Tress Classifier)\"\n",
    "# Cross validation with 100 iterations to get smoother mean test and train\n",
    "# score curves, each time with 20% data randomly selected as a validation set.\n",
    "cv = ShuffleSplit(n_splits=20,test_size=0.3, random_state=0)\n",
    "estimator = BalancedBaggingClassifier(random_state=0,n_estimators=100,base_estimator=ExtraTreesClassifier(class_weight='balanced'))\n",
    "plot_learning_curve(estimator,title, X, y, ylim=(0.4, 1.01), cv=cv, n_jobs=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import average_precision_score\n",
    "# average_precision = average_precision_score(y_test, predictions)\n",
    "# print('Average precision-recall score: {0:0.2f}'.format(average_precision))\n",
    "# from sklearn.metrics import precision_recall_curve\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# precision, recall, _ = precision_recall_curve(y_test,predictions)\n",
    "\n",
    "# plt.step(recall, precision, color='b', alpha=0.2,where='post')\n",
    "# plt.fill_between(recall, precision, step='post', alpha=0.2,color='b')\n",
    "\n",
    "# plt.xlabel('Recall')\n",
    "# plt.ylabel('Precision')\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.\n",
    "#           format(average_precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = ExtraTreesClassifier(class_weight='balanced')\n",
    "# model.fit(X_train_res, y_train_res)\n",
    "# predictions_train = model.predict_proba(X_train_res)[:,1]\n",
    "# print(\"Train ROC-AUC :\",roc_auc_score(y_train_res, predictions_train))\n",
    "# #print(\"Train Classification Report : \\n\",classification_report(y_train_res,predictions_train))\n",
    "# predictions = model.predict_proba(X_test)[:,1]\n",
    "# print(\"Test ROC-AUC :\",roc_auc_score(y_test, predictions))\n",
    "# plt.hist(predictions,log=True)\n",
    "# #print(\"Train Classification Report : \\n\",classification_report(y_test, predictions))\n",
    "# #print(\"Test Confusion Matrix :\\n \")\n",
    "# # cnf_matrix = confusion_matrix(y_test,predictions)\n",
    "# # plt.figure()\n",
    "# # plot_confusion_matrix(cnf_matrix, classes=[0,1],title='Confusion matrix, without normalization')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(predictions,log=True)\n",
    "# print(np.median(predictions))\n",
    "# print(np.mean(predictions))\n",
    "# print(np.percentile(predictions,q=25))\n",
    "# print(np.percentile(predictions,q=50))\n",
    "# print(np.percentile(predictions,q=75))\n",
    "# #print(\"Train Classification Report : \\n\",classification_report(y_test, predictions))\n",
    "# #print(\"Test Confusion Matrix :\\n \")\n",
    "# # cnf_matrix = confusion_matrix(y_test,predictions)\n",
    "# # plt.figure()\n",
    "# # plot_confusion_matrix(cnf_matrix, classes=[0,1],title='Confusion matrix, without normalization')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "# Author: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n",
    "# License: BSD Style.\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=100000, n_features=20,\n",
    "                                    n_informative=2, n_redundant=2)\n",
    "\n",
    "train_samples = 100  # Samples used for training the models\n",
    "\n",
    "X_train = X_train_res\n",
    "X_test = X_test\n",
    "y_train = y_train_res\n",
    "y_test = y_test\n",
    "\n",
    "# Create classifiers\n",
    "lr = LogisticRegression(class_weight='balanced')\n",
    "gnb = ExtraTreesClassifier(class_weight='balanced')\n",
    "rfc = RandomForestClassifier(n_estimators=100,class_weight='balanced')\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Plot calibration plots\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "for clf, name in [(lr, 'Logistic'),(gnb, 'Naive Bayes'),(rfc, 'Random Forest')]:\n",
    "    clf.fit(X_train, y_train)\n",
    "    if hasattr(clf, \"predict_proba\"):\n",
    "        prob_pos = clf.predict_proba(X_test)[:, 1]\n",
    "    else:  # use decision function\n",
    "        prob_pos = clf.decision_function(X_test)\n",
    "        prob_pos = \\\n",
    "            (prob_pos - prob_pos.min()) / (prob_pos.max() - prob_pos.min())\n",
    "    fraction_of_positives, mean_predicted_value = \\\n",
    "        calibration_curve(y_test, prob_pos, n_bins=10)\n",
    "\n",
    "    ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n",
    "             label=\"%s\" % (name, ))\n",
    "\n",
    "    ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n",
    "             histtype=\"step\", lw=2)\n",
    "\n",
    "ax1.set_ylabel(\"Fraction of positives\")\n",
    "ax1.set_ylim([-0.05, 1.05])\n",
    "ax1.legend(loc=\"lower right\")\n",
    "ax1.set_title('Calibration plots  (reliability curve)')\n",
    "\n",
    "ax2.set_xlabel(\"Mean predicted value\")\n",
    "ax2.set_ylabel(\"Count\")\n",
    "ax2.legend(loc=\"upper center\", ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "def findOptimalCutoff(target, predicted):\n",
    "    \"\"\" Find the optimal probability cutoff point for a classification model related to event rate\n",
    "    Parameters\n",
    "    ----------\n",
    "    target : Matrix with dependent or target data, where rows are observations\n",
    "\n",
    "    predicted : Matrix with predicted data, where rows are observations\n",
    "\n",
    "    Returns\n",
    "    -------     \n",
    "    list type, with optimal cutoff value\n",
    "\n",
    "    \"\"\"\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr)) \n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index=i), 'threshold' : pd.Series(threshold, index=i)})\n",
    "    roc_t = roc.ix[(roc.tf-0).abs().argsort()[:1]]\n",
    "    return list(roc_t['threshold']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (findOptimalCutoff(y_train_res,predictions_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from itertools import cycle\n",
    "\n",
    "# from sklearn import svm, datasets\n",
    "# from sklearn.metrics import roc_curve, auc\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import label_binarize\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "# from scipy import interp\n",
    "\n",
    "# # Binarize the output\n",
    "# y = label_binarize(y, classes=[0, 1])\n",
    "# n_classes = y.shape[1]\n",
    "\n",
    "# # Add noisy features to make the problem harder\n",
    "# random_state = np.random.RandomState(0)\n",
    "# n_samples, n_features = X.shape\n",
    "# X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# # shuffle and split training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,random_state=0)\n",
    "\n",
    "# # Learn to predict each class against the other\n",
    "# classifier = ExtraTreesClassifier(class_weight='balanced')\n",
    "# y_score = classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Compute ROC curve and ROC area for each class\n",
    "# fpr = dict()\n",
    "# tpr = dict()\n",
    "# roc_auc = dict()\n",
    "# for i in range(n_classes):\n",
    "#     fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "#     roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# # Compute micro-average ROC curve and ROC area\n",
    "# fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n",
    "# roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "# plt.figure()\n",
    "# lw = 2\n",
    "# plt.plot(fpr[2], tpr[2], color='darkorange',\n",
    "#          lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n",
    "# plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Receiver operating characteristic example')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
